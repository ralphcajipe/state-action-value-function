# state-action-value-function
Reinforcement Learning Exploration with Mars Rover Example

## QSA Values Exploration

In the field of reinforcement learning, understanding the State-Action Value function (QSA) is crucial for making informed decisions in dynamic environments. This README guides you through a hands-on exploration using a Mars Rover example.

### Key Concepts:

#### QSA Representation

- QSA signifies the value of taking action 'A' in state 'S'.
- It is essential for evaluating optimal actions in reinforcement learning scenarios.

#### Jupyter Notebook Exploration

- Utilize the provided Jupyter Notebook for interactive exploration.
- The Mars Rover example serves as a practical application to delve into QSA values.

#### Rewards and Discount Factors

- Modify rewards and discount factors.
- Adjusting terminal right rewards directly influences optimal policies.
- Lower right rewards shift the Rover's preference towards the left.

#### Discount Factor Impact

- Tweaking the discount factor influences the agent's patience.
- A higher discount factor (closer to 1) implies more patience for delayed, higher rewards.
- A lower discount factor (closer to 0) leads to impatience, favoring immediate, lower rewards.

#### Optimal Policy Changes

- Optimal policies dynamically change based on reward adjustments and discount factors.
- The demonstration showcases the impact of parameter alterations on QSA values and optimal returns.

#### Bellman Equation Teaser

- Hinting at the importance of the Bellman equation in reinforcement learning.
- Further exploration and experimentation with the provided Jupyter Notebook can enhance your understanding of these fundamental concepts.

## How to Use the Code

1. Experiment with modifying rewards, discount factors, and other parameters.
2. Observe and analyze how QSA values and optimal policies evolve under different scenarios.
